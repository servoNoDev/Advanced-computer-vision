{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Advanced Computer Vision - Week_03 - Image segmentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import KFold\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T16:16:15.096894Z",
     "start_time": "2024-02-28T16:16:15.092210Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Jaccard Loss and Dice Loss Explanation\n",
    "### Binary Cross-Entropy\n",
    "\n",
    "Binary Cross-Entropy (BCE) loss is a commonly used loss function for binary classification tasks. It measures the dissimilarity between two probability distributions, typically the predicted probabilities and the true labels.\n",
    "\n",
    "The BCE loss is calculated as follows:\n",
    "\n",
    "For each prediction, it applies the sigmoid activation function to ensure the predicted values are in the range [0, 1], representing probabilities.\n",
    "Then, it computes the Binary Cross-Entropy loss between the predicted probabilities and the true binary labels.\n",
    "BCE loss penalizes the model based on the difference between predicted probabilities and true labels. It encourages the model to output probabilities close to 1 for positive examples and close to 0 for negative examples.\n",
    "\n",
    "### Jaccard Loss:\n",
    "The Jaccard index, also known as the Intersection over Union (IoU), is a measure of the similarity between two sets. In the context of segmentation tasks (such as image segmentation), the Jaccard index quantifies the overlap between the predicted segmentation and the ground truth segmentation.\n",
    "\n",
    "Jaccard loss is a loss function derived from the Jaccard index. It measures the dissimilarity between two sets by calculating the ratio of the intersection of the sets to their union. The Jaccard loss is computed as: Jaccard Loss = 1 - (Intersection / Union)\n",
    "\n",
    "\n",
    "Where:\n",
    "- **Intersection**: Number of common elements between the predicted segmentation and the ground truth segmentation.\n",
    "- **Union**: Total number of elements in both the predicted and ground truth segmentations.\n",
    "\n",
    "The Jaccard loss penalizes the model when the predicted segmentation deviates from the ground truth segmentation, encouraging the model to produce segmentations with higher overlap with the ground truth.\n",
    "\n",
    "### Dice Loss:\n",
    "\n",
    "The Dice coefficient, also known as the Sørensen-Dice coefficient, is another measure of the similarity between two sets. Like the Jaccard index, it is commonly used in segmentation tasks to evaluate the overlap between the predicted segmentation and the ground truth segmentation.\n",
    "\n",
    "Dice loss is a loss function derived from the Dice coefficient. It is calculated as:\n",
    "Dice Loss = 1 - (2 * Intersection + ε) / (Total Predicted + Total Targets + ε)\n",
    "\n",
    "Where:\n",
    "- **Intersection**: Number of common elements between the predicted segmentation and the ground truth segmentation.\n",
    "- **Total Predicted**: Total number of elements in the predicted segmentation.\n",
    "- **Total Targets**: Total number of elements in the ground truth segmentation.\n",
    "- **ε**: A small constant added to the denominator to avoid division by zero.\n",
    "\n",
    "Similar to the Jaccard loss, the Dice loss encourages the model to produce segmentations with higher overlap with the ground truth. It penalizes deviations between the predicted and ground truth segmentations by measuring the dissimilarity between them.\n",
    "\n",
    "Both Jaccard loss and Dice loss are commonly used as loss functions in tasks involving segmentation, such as medical image analysis, object detection, and scene understanding. They are effective in guiding the training process towards producing accurate segmentations.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class JaccardLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the JaccardLoss module.\n",
    "        \"\"\"\n",
    "        super(JaccardLoss, self).__init__()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Calculates the Jaccard loss between predictions and targets.\n",
    "\n",
    "        Args:\n",
    "            predictions (torch.Tensor): Predicted values.\n",
    "            targets (torch.Tensor): Target values.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Computed Jaccard loss.\n",
    "        \"\"\"\n",
    "        # Calculate the intersection of predictions and targets\n",
    "        intersection = torch.sum(predictions * targets)\n",
    "        # Calculate the union of predictions and targets\n",
    "        union = torch.sum(predictions + targets) - intersection\n",
    "        # Calculate the Jaccard index with a small epsilon to avoid division by zero\n",
    "        jaccard = (intersection + 1e-5) / (union + 1e-5)\n",
    "        # Calculate the Jaccard loss\n",
    "        jaccard_loss = 1 - jaccard\n",
    "        return jaccard_loss\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the DiceLoss module.\n",
    "        \"\"\"\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Calculates the Dice loss between predictions and targets.\n",
    "\n",
    "        Args:\n",
    "            predictions (torch.Tensor): Predicted values.\n",
    "            targets (torch.Tensor): Target values.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Computed Dice loss.\n",
    "        \"\"\"\n",
    "        # Calculate the intersection of predictions and targets\n",
    "        intersection = torch.sum(predictions * targets)\n",
    "        # Calculate the Dice coefficient with a small epsilon to avoid division by zero\n",
    "        dice_coefficient = (2.0 * intersection + 1e-5) / (torch.sum(predictions) + torch.sum(targets) + 1e-5)\n",
    "        # Calculate the Dice loss\n",
    "        dice_loss = 1 - dice_coefficient\n",
    "        return dice_loss\n",
    "\n",
    "class BinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the BinaryCrossEntropyLoss module.\n",
    "        \"\"\"\n",
    "        super(BinaryCrossEntropyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Calculates the Binary Cross-Entropy loss between predictions and targets.\n",
    "\n",
    "        Args:\n",
    "            predictions (torch.Tensor): Predicted values.\n",
    "            targets (torch.Tensor): Target values.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Computed Binary Cross-Entropy loss.\n",
    "        \"\"\"\n",
    "        # Apply sigmoid activation to predictions to ensure they are in the range [0, 1]\n",
    "        predictions = torch.sigmoid(predictions)\n",
    "        # Compute the Binary Cross-Entropy loss\n",
    "        bce_loss = nn.BCELoss()(predictions, targets)\n",
    "        return bce_loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T16:24:09.714020Z",
     "start_time": "2024-02-28T16:24:09.712656Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, fold, transform=None, train=True, tp=0):\n",
    "        self.data_frame = pd.read_csv(csv_file)[[\"video_id_x\", \"frame_cropped_path\", \"mask_cropped_path\", \"polygon_label\"]]\n",
    "        if tp == 0:\n",
    "            self.data_frame = self.data_frame[self.data_frame[\"polygon_label\"].isin([\"lungslidingpresent\", \"lungslidingabsent\"])]\n",
    "        elif tp == 1:\n",
    "            self.data_frame = self.data_frame[self.data_frame[\"polygon_label\"] == \"aline\"]\n",
    "        elif tp == 2:\n",
    "            self.data_frame = self.data_frame[self.data_frame[\"polygon_label\"] == \"bline\"]\n",
    "        self.transform = transform\n",
    "\n",
    "        # Create folds based on patient_id\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        patient_ids = self.data_frame['video_id_x'].values\n",
    "        for fold_num, (train_index, val_index) in enumerate(kf.split(X=range(len(patient_ids)))):\n",
    "            if fold_num == fold:\n",
    "                if train:\n",
    "                    self.data_frame = self.data_frame.iloc[train_index]\n",
    "                else:\n",
    "                    self.data_frame = self.data_frame.iloc[val_index]\n",
    "                break\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data_frame.iloc[idx, 1]\n",
    "        mask_path = self.data_frame.iloc[idx, 2]\n",
    "        image = Image.open(f\"{PATH}/\"+img_path)\n",
    "        mask = Image.open(f\"{PATH}/\"+mask_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return [image, mask]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T16:25:37.029652Z",
     "start_time": "2024-02-28T16:25:36.993427Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "PATH = \"/Users/hlibokymaros/Documents/_datasets/APVV_Lung/revision_8\"\n",
    "def getDataset(batch_size=256, workers=16, fold=0, tp=0):\n",
    "    transform = transforms.Compose([  # Transform for test set\n",
    "        transforms.Resize((512,512)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    trainset = CustomDataset(f\"{PATH}/frames_label_full_final_all.csv\", fold, transform=transform, tp=tp)\n",
    "    testset = CustomDataset(f\"{PATH}/frames_label_full_final_all.csv\", fold, transform=transform, train=False, tp=tp)\n",
    "\n",
    "    #trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, num_workers=8, sampler=sampler_init)\n",
    "    # loader for the training set\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, num_workers=workers, shuffle=True)\n",
    "    # loader for the training set\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=workers)\n",
    "    # loader for the testing set\n",
    "    return {\"train\": trainloader, \"val\": testloader}  # set of loaders\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T16:25:37.289240Z",
     "start_time": "2024-02-28T16:25:37.138605Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder1 = self.conv_block(1, 64)\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.encoder4 = self.conv_block(256, 512)\n",
    "\n",
    "        # Middle (bottleneck)\n",
    "        self.middle = self.deconv_block(512, 1024)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder4 = self.deconv_block(1024, 512)\n",
    "        self.decoder3 = self.deconv_block(512, 256)\n",
    "        self.decoder2 = self.deconv_block(256, 128)\n",
    "        self.decoder1 = self.last_deconv_block(128, 64)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=\"same\"),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=\"same\"),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def deconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=\"same\"),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=\"same\"),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(out_channels, out_channels//2, kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "\n",
    "    def last_deconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=\"same\"),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=\"same\"),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.maxpool(enc1))\n",
    "        enc3 = self.encoder3(self.maxpool(enc2))\n",
    "        enc4 = self.encoder4(self.maxpool(enc3))\n",
    "\n",
    "        # Middle (bottleneck)\n",
    "        middle = self.middle(self.maxpool(enc4))\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        dec4 = torch.cat([enc4, middle], dim=1)  # Skip connection\n",
    "        dec4 = self.decoder4(dec4)\n",
    "\n",
    "        dec3 = torch.cat([enc3, dec4], dim=1)  # Skip connection\n",
    "        dec3 = self.decoder3(dec3)\n",
    "\n",
    "        dec2 = torch.cat([enc2, dec3], dim=1)  # Skip connection\n",
    "        dec2 = self.decoder2(dec2)\n",
    "\n",
    "        dec1 = torch.cat([enc1, dec2], dim=1)  # Skip connection\n",
    "        output = self.decoder1(dec1)\n",
    "\n",
    "        return output\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T16:25:37.956155Z",
     "start_time": "2024-02-28T16:25:37.321601Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('mps')\n",
    "device = torch.device('cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T16:36:31.151963Z",
     "start_time": "2024-02-28T16:36:31.095694Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# def train(device, lr=0.0001, fold=0, tp=0, loss_type=0):\n",
    "lr=0.0001\n",
    "fold=0\n",
    "tp=0\n",
    "loss_type=0\n",
    "    # wandb.login()\n",
    "batch_size = 2**4\n",
    "loaders = getDataset(batch_size, fold, tp)  # loader for dataset\n",
    "\n",
    "\n",
    "\n",
    "model = UNet().to(device)  # capsule model\n",
    "\n",
    "\n",
    "\n",
    "#model = smp.Unet().to(device)  # capsule model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # optimizer\n",
    "lr_decay = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.98)  # weight decay, which is used in capsules\n",
    "\n",
    "bce = nn.BCELoss().to(device)\n",
    "iou = JaccardLoss().to(device)\n",
    "dice = DiceLoss().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T16:36:31.602763Z",
     "start_time": "2024-02-28T16:36:31.102634Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 7/396 [09:16<8:35:02, 79.44s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[46], line 33\u001B[0m\n\u001B[1;32m     31\u001B[0m caps \u001B[38;5;241m=\u001B[39m model(inputs)  \u001B[38;5;66;03m# get output from capsule model.  Output is [batch, classes, caps_dim]\u001B[39;00m\n\u001B[1;32m     32\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(caps, labels)  \u001B[38;5;66;03m# classification error\u001B[39;00m\n\u001B[0;32m---> 33\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# calculating gradients\u001B[39;00m\n\u001B[1;32m     34\u001B[0m ls\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mitem())\n\u001B[1;32m     35\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()  \u001B[38;5;66;03m# changing weights\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/KKUI-PPV_LABs/lib/python3.9/site-packages/torch/_tensor.py:488\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    480\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    481\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    486\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    487\u001B[0m     )\n\u001B[0;32m--> 488\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/KKUI-PPV_LABs/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if loss_type == 0:\n",
    "    criterion = bce  # loss function for classification\n",
    "elif loss_type == 1:\n",
    "    criterion = iou\n",
    "elif loss_type == 2:\n",
    "    criterion = dice\n",
    "val_loss_min = np.inf  # variable to keep the performance of the best model\n",
    "counter = 10\n",
    "\n",
    "types = [\"Pleura\", \"Aline\", \"Bline\"]\n",
    "losses = [\"BCE\", \"IOU\", \"DICE\"]\n",
    "\n",
    "# wandb.init(\n",
    "#     project=\"Lung\",\n",
    "#\n",
    "#     config={\n",
    "#         \"learning_rate\": lr,\n",
    "#         \"Model\": \"Unet\",\n",
    "#         \"fold\": fold,\n",
    "#         \"type\": types[tp],\n",
    "#         \"loss_type\": losses[loss_type]\n",
    "#     }\n",
    "# )\n",
    "\n",
    "for epoch in range(100):\n",
    "    start = time.time()\n",
    "    model.train()  # change model to train mode\n",
    "    ls = []  # empty list to save classification loss\n",
    "    for inputs, labels in tqdm(loaders[\"train\"]):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        caps = model(inputs)  # get output from capsule model.  Output is [batch, classes, caps_dim]\n",
    "        loss = criterion(caps, labels)  # classification error\n",
    "        loss.backward()  # calculating gradients\n",
    "        ls.append(loss.detach().cpu().item())\n",
    "        optimizer.step()  # changing weights\n",
    "        optimizer.zero_grad()\n",
    "    lr_decay.step()  # learning rate decayed by gamma: lr = lr * gamma\n",
    "    ls = sum(ls)/len(ls)\n",
    "    bce_val = []\n",
    "    iou_val = []\n",
    "    dice_val = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loaders[\"val\"]:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            caps = model(inputs)\n",
    "            bce_val.append(bce(caps, labels).detach().cpu().item())\n",
    "            iou_val.append(iou(caps, labels).detach().cpu().item())\n",
    "            dice_val.append(dice(caps, labels).detach().cpu().item())\n",
    "    bce_val = sum(bce_val)/len(bce_val)\n",
    "    iou_val = sum(iou_val)/len(iou_val)\n",
    "    dice_val = sum(dice_val)/len(dice_val)\n",
    "    if loss_type == 0:\n",
    "        val_loss = bce_val\n",
    "    elif loss_type == 1:\n",
    "        val_loss = iou_val\n",
    "    elif loss_type == 2:\n",
    "        val_loss = dice_val\n",
    "    if val_loss_min > val_loss:\n",
    "        counter = 10\n",
    "        val_loss_min = val_loss\n",
    "        torch.save(model.state_dict(), str(fold)+\"_\" + str(tp) +\"_\" +str(loss_type) +\".mo\")\n",
    "    print(\n",
    "        \"Epoch %d, train_loss %4.4f, bce_val %4.4f, iou_val %4.4f, dice_val %4.4f, time %4.2f\" % (\n",
    "            epoch, ls, bce_val, iou_val, dice_val, time.time() - start))\n",
    "    # wandb.log({\"train_loss\": ls, \"val_loss\": val_loss, \"bce_val\": bce_val, \"iou_val\": iou_val, \"dice_val\": dice_val, \"time\": time.time() - start})\n",
    "    if counter == 0:\n",
    "        break\n",
    "    counter -= 1\n",
    "# wandb.config[\"final_loss\"] = val_loss_min\n",
    "# wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T16:45:47.813025Z",
     "start_time": "2024-02-28T16:36:31.612857Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    cd = int(sys.argv[1])\n",
    "    device = \"cuda:\" + sys.argv[1]\n",
    "    for i in range(5):\n",
    "        for j in range(3):\n",
    "            for k in range(3):\n",
    "                #if i*9+j*3+k % 2 == cd:\n",
    "                seed = 42\n",
    "                torch.manual_seed(seed)\n",
    "                torch.cuda.manual_seed(seed)\n",
    "                torch.backends.cudnn.deterministic = True\n",
    "                torch.backends.cudnn.benchmark = False\n",
    "                np.random.seed(seed)\n",
    "                #pretrained = int(sys.argv[1]) > 0\n",
    "                train(fold=i, tp=j, loss_type=k)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
