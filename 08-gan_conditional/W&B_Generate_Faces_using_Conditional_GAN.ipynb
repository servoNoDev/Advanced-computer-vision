{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY9FTrVZtsjf"
      },
      "source": [
        "# Face Generation using Conditional GANs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFzRQYnCt5rj"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x9uBL4Nk5OcH"
      },
      "outputs": [],
      "source": [
        "# !pip install -q --upgrade wandb\n",
        "\n",
        "# # Wandb Login\n",
        "# import wandb\n",
        "# wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ObwAnCv7m4_Z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n",
            "ERROR: could not open HSTS store at '/home/mh731nk/.wget-hsts'. HSTS will be disabled.\n",
            "--2024-04-03 18:38:25--  https://www.dropbox.com/s/rbajpdlh7efkdo1/male_female_face_images.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.72.18, 2620:100:6027:18::a27d:4812\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.72.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/rbajpdlh7efkdo1/male_female_face_images.zip [following]\n",
            "--2024-04-03 18:38:25--  https://www.dropbox.com/s/raw/rbajpdlh7efkdo1/male_female_face_images.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc821331f70e8331dd5311d0cf91.dl.dropboxusercontent.com/cd/0/inline/CQWRY0xkBmGLZUgUg-FLrqgrZYOX1fbgMz9wMwWo7CPPUAycpryTrKfJG2KnRglARXxTftlM8otscIqoVCZ1OS17elFm0vfeCu-yogTYT_5DAiQr23HM_CpvEsEsfkYnU9PE6UlNwQZAphzh-2ny7MaD/file# [following]\n",
            "--2024-04-03 18:38:26--  https://uc821331f70e8331dd5311d0cf91.dl.dropboxusercontent.com/cd/0/inline/CQWRY0xkBmGLZUgUg-FLrqgrZYOX1fbgMz9wMwWo7CPPUAycpryTrKfJG2KnRglARXxTftlM8otscIqoVCZ1OS17elFm0vfeCu-yogTYT_5DAiQr23HM_CpvEsEsfkYnU9PE6UlNwQZAphzh-2ny7MaD/file\n",
            "Resolving uc821331f70e8331dd5311d0cf91.dl.dropboxusercontent.com (uc821331f70e8331dd5311d0cf91.dl.dropboxusercontent.com)... 162.125.72.15, 2620:100:6027:15::a27d:480f\n",
            "Connecting to uc821331f70e8331dd5311d0cf91.dl.dropboxusercontent.com (uc821331f70e8331dd5311d0cf91.dl.dropboxusercontent.com)|162.125.72.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CQUZG5Ng-p7XggQ_o_pZOQsH6kxLFUSiNsAjV2LnqrE6FN2CsBjXDUaJJiztNeYHcDShLxUxkT2MokL621Ed8S1VxLQARkSgZoFf9MXfl2yawxCquyvY1N30c2uLvaZnQ31bW0tB564MgXEAPh38QjzZav9sParEHBasJBnS15mKyP-1SSDsg-zpevP9rbRFJWTpbkdfuN_rW2umd1cFhmvGoo1oKcf_mPlSfSyCs-1AW7xsBXa9hqxydVxFLPL2ect7zAlq7NOra9PRmh257U_innVl8WR3bsWbVvTZHuafK6wIi7LqOJZect5uXQ9KgaMkU1u_JFWcRlWyb9QgmnUWrRA4KlPHMafY6zFKg8J7hOBhMgM2AoiioHRHaxOPe5Q/file [following]\n",
            "--2024-04-03 18:38:27--  https://uc821331f70e8331dd5311d0cf91.dl.dropboxusercontent.com/cd/0/inline2/CQUZG5Ng-p7XggQ_o_pZOQsH6kxLFUSiNsAjV2LnqrE6FN2CsBjXDUaJJiztNeYHcDShLxUxkT2MokL621Ed8S1VxLQARkSgZoFf9MXfl2yawxCquyvY1N30c2uLvaZnQ31bW0tB564MgXEAPh38QjzZav9sParEHBasJBnS15mKyP-1SSDsg-zpevP9rbRFJWTpbkdfuN_rW2umd1cFhmvGoo1oKcf_mPlSfSyCs-1AW7xsBXa9hqxydVxFLPL2ect7zAlq7NOra9PRmh257U_innVl8WR3bsWbVvTZHuafK6wIi7LqOJZect5uXQ9KgaMkU1u_JFWcRlWyb9QgmnUWrRA4KlPHMafY6zFKg8J7hOBhMgM2AoiioHRHaxOPe5Q/file\n",
            "Reusing existing connection to uc821331f70e8331dd5311d0cf91.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 267204277 (255M) [application/zip]\n",
            "Saving to: ‘male_female_face_images.zip’\n",
            "\n",
            "male_female_face_im 100%[===================>] 254.83M  33.1MB/s    in 8.7s    \n",
            "\n",
            "2024-04-03 18:38:37 (29.2 MB/s) - ‘male_female_face_images.zip’ saved [267204277/267204277]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.dropbox.com/s/rbajpdlh7efkdo1/male_female_face_images.zip\n",
        "!unzip -q male_female_face_images.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX08hMBtnDeW",
        "outputId": "246c21ca-cca1-452b-924a-667b43f5e76f"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade torch_snippets\n",
        "from torch_snippets import *\n",
        "\n",
        "import torch\n",
        "from torchvision.utils import make_grid\n",
        "from torch_snippets import *\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwo-mhJOuEok"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlabbJUcuFta"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kkJk7iOmpp1Q"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# wandb config\n",
        "WANDB_CONFIG = {\n",
        "              '_wandb_kernel': 'neuracort'\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t94Iw5qVuHRh"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG28vYxKuOd9"
      },
      "source": [
        "## Crop Images to Obtain Faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6eXBqx39nH80"
      },
      "outputs": [],
      "source": [
        "female_images = Glob('/content/females/*.jpg')\n",
        "male_images = Glob('/content/males/*.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Oq_QuchQnJrk"
      },
      "outputs": [],
      "source": [
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LvKcMegYnLuf"
      },
      "outputs": [],
      "source": [
        "!mkdir cropped_faces_female\n",
        "!mkdir cropped_faces_male\n",
        "\n",
        "for i in range(len(female_images)):\n",
        "    img = read(female_images[i],1)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "    for (x,y,w,h) in faces:\n",
        "        img2 = img[y:(y+h),x:(x+w),:]\n",
        "    cv2.imwrite('cropped_faces_female/'+str(i)+'.jpg',cv2.cvtColor(img2, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "for i in range(len(male_images)):\n",
        "    img = read(male_images[i],1)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "    for (x,y,w,h) in faces:\n",
        "        img2 = img[y:(y+h),x:(x+w),:]\n",
        "    cv2.imwrite('cropped_faces_male/'+str(i)+'.jpg',cv2.cvtColor(img2, cv2.COLOR_RGB2BGR))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "303-1FDbuUTc"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hVNfdnauU-d"
      },
      "source": [
        "## Apply Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4si0y8F-nNqC"
      },
      "outputs": [],
      "source": [
        "transform=transforms.Compose([\n",
        "                               transforms.Resize(64),\n",
        "                               transforms.CenterCrop(64),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apDdE0F4uXEB"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0ETSEA0uYKI"
      },
      "source": [
        "## Define the Dataset Class and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Q-_lQ0JSo7RO"
      },
      "outputs": [],
      "source": [
        "class Faces(Dataset):\n",
        "    def __init__(self, folders):\n",
        "        super().__init__()\n",
        "        self.folderfemale = folders[0]\n",
        "        self.foldermale = folders[1]\n",
        "        self.images=sorted(Glob(self.folderfemale))+sorted(Glob(self.foldermale))\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    def __getitem__(self, ix):\n",
        "        image_path = self.images[ix]\n",
        "        image = Image.open(image_path)\n",
        "        image = transform(image)\n",
        "        gender = np.where('female' in str(image_path),1,0)\n",
        "        return image, torch.tensor(gender).long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-3zgclepGlv",
        "outputId": "e63f3299-281c-4b19-af1c-dec43b62b6e7"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "num_samples should be a positive integer value, but got num_samples=0",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ds \u001b[38;5;241m=\u001b[39m Faces(folders\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcropped_faces_female\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcropped_faces_male\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/mh731nk_apvv-lung-eval/lib/python3.9/site-packages/torch/utils/data/dataloader.py:351\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 351\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/mh731nk_apvv-lung-eval/lib/python3.9/site-packages/torch/utils/data/sampler.py:107\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue, but got num_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples))\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ],
      "source": [
        "ds = Faces(folders=['cropped_faces_female','cropped_faces_male'])\n",
        "dataloader = DataLoader(ds, batch_size=64, shuffle=True, num_workers=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b5qlcFtub6R"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocToFbXiuckg"
      },
      "source": [
        "## Weight Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C184MaaTpQcr"
      },
      "outputs": [],
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loaXeLsVuej-"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVkVikz4ufN_"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cALu7xyMpSjw"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, emb_size=32):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.emb_size = 32\n",
        "        self.label_embeddings = nn.Embedding(2, self.emb_size)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3,64,4,2,1,bias=False),\n",
        "            nn.LeakyReLU(0.2,inplace=True),\n",
        "            nn.Conv2d(64,64*2,4,2,1,bias=False),\n",
        "            nn.BatchNorm2d(64*2),\n",
        "            nn.LeakyReLU(0.2,inplace=True),\n",
        "            nn.Conv2d(64*2,64*4,4,2,1,bias=False),\n",
        "            nn.BatchNorm2d(64*4),\n",
        "            nn.LeakyReLU(0.2,inplace=True),\n",
        "            nn.Conv2d(64*4,64*8,4,2,1,bias=False),\n",
        "            nn.BatchNorm2d(64*8),\n",
        "            nn.LeakyReLU(0.2,inplace=True),\n",
        "            nn.Conv2d(64*8,64,4,2,1,bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2,inplace=True),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.model2 = nn.Sequential(\n",
        "            nn.Linear(288,100),\n",
        "            nn.LeakyReLU(0.2,inplace=True),\n",
        "            nn.Linear(100,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.apply(weights_init)\n",
        "    def forward(self, input, labels):\n",
        "        x = self.model(input)\n",
        "        y = self.label_embeddings(labels)\n",
        "        input = torch.cat([x, y], 1)\n",
        "        final_output = self.model2(input)\n",
        "        return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWkFJ6eBpU3c",
        "outputId": "1941351d-eaf6-408e-8343-465f5e4e533a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_summary\n",
            "  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: torch-summary\n",
            "Successfully installed torch-summary-1.4.5\n",
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "├─Sequential: 1-1                        [-1, 256]                 --\n",
            "|    └─Conv2d: 2-1                       [-1, 64, 32, 32]          3,072\n",
            "|    └─LeakyReLU: 2-2                    [-1, 64, 32, 32]          --\n",
            "|    └─Conv2d: 2-3                       [-1, 128, 16, 16]         131,072\n",
            "|    └─BatchNorm2d: 2-4                  [-1, 128, 16, 16]         256\n",
            "|    └─LeakyReLU: 2-5                    [-1, 128, 16, 16]         --\n",
            "|    └─Conv2d: 2-6                       [-1, 256, 8, 8]           524,288\n",
            "|    └─BatchNorm2d: 2-7                  [-1, 256, 8, 8]           512\n",
            "|    └─LeakyReLU: 2-8                    [-1, 256, 8, 8]           --\n",
            "|    └─Conv2d: 2-9                       [-1, 512, 4, 4]           2,097,152\n",
            "|    └─BatchNorm2d: 2-10                 [-1, 512, 4, 4]           1,024\n",
            "|    └─LeakyReLU: 2-11                   [-1, 512, 4, 4]           --\n",
            "|    └─Conv2d: 2-12                      [-1, 64, 2, 2]            524,288\n",
            "|    └─BatchNorm2d: 2-13                 [-1, 64, 2, 2]            128\n",
            "|    └─LeakyReLU: 2-14                   [-1, 64, 2, 2]            --\n",
            "|    └─Flatten: 2-15                     [-1, 256]                 --\n",
            "├─Embedding: 1-2                         [-1, 32]                  64\n",
            "├─Sequential: 1-3                        [-1, 1]                   --\n",
            "|    └─Linear: 2-16                      [-1, 100]                 28,900\n",
            "|    └─LeakyReLU: 2-17                   [-1, 100]                 --\n",
            "|    └─Linear: 2-18                      [-1, 1]                   101\n",
            "|    └─Sigmoid: 2-19                     [-1, 1]                   --\n",
            "==========================================================================================\n",
            "Total params: 3,310,857\n",
            "Trainable params: 3,310,857\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 109.25\n",
            "==========================================================================================\n",
            "Input size (MB): 1.50\n",
            "Forward/backward pass size (MB): 1.38\n",
            "Params size (MB): 12.63\n",
            "Estimated Total Size (MB): 15.51\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_summary\n",
        "from torchsummary import summary\n",
        "discriminator = Discriminator().to(device)\n",
        "summary(discriminator,torch.zeros(32,3,64,64).to(device), torch.zeros(32).long().to(device));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZUYdbP2uhSc"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVQmLj5SuiH2"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vJXr2BEpZAZ"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, emb_size=32):\n",
        "        super(Generator,self).__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.label_embeddings = nn.Embedding(2, self.emb_size)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.ConvTranspose2d(100+self.emb_size,64*8,4,1,0,bias=False),\n",
        "            nn.BatchNorm2d(64*8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64*8,64*4,4,2,1,bias=False),\n",
        "            nn.BatchNorm2d(64*4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64*4,64*2,4,2,1,bias=False),\n",
        "            nn.BatchNorm2d(64*2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64*2,64,4,2,1,bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64,3,4,2,1,bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.apply(weights_init)\n",
        "    def forward(self,input_noise,labels):\n",
        "        label_embeddings = self.label_embeddings(labels).view(len(labels), self.emb_size, 1, 1)\n",
        "        input = torch.cat([input_noise, label_embeddings], 1)\n",
        "        return self.model(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIJXW1SxphF7",
        "outputId": "3d84fc4d-875e-4a2d-8497-4cde175c3b5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "├─Embedding: 1-1                         [-1, 32]                  64\n",
            "├─Sequential: 1-2                        [-1, 3, 64, 64]           --\n",
            "|    └─ConvTranspose2d: 2-1              [-1, 512, 4, 4]           1,081,344\n",
            "|    └─BatchNorm2d: 2-2                  [-1, 512, 4, 4]           1,024\n",
            "|    └─ReLU: 2-3                         [-1, 512, 4, 4]           --\n",
            "|    └─ConvTranspose2d: 2-4              [-1, 256, 8, 8]           2,097,152\n",
            "|    └─BatchNorm2d: 2-5                  [-1, 256, 8, 8]           512\n",
            "|    └─ReLU: 2-6                         [-1, 256, 8, 8]           --\n",
            "|    └─ConvTranspose2d: 2-7              [-1, 128, 16, 16]         524,288\n",
            "|    └─BatchNorm2d: 2-8                  [-1, 128, 16, 16]         256\n",
            "|    └─ReLU: 2-9                         [-1, 128, 16, 16]         --\n",
            "|    └─ConvTranspose2d: 2-10             [-1, 64, 32, 32]          131,072\n",
            "|    └─BatchNorm2d: 2-11                 [-1, 64, 32, 32]          128\n",
            "|    └─ReLU: 2-12                        [-1, 64, 32, 32]          --\n",
            "|    └─ConvTranspose2d: 2-13             [-1, 3, 64, 64]           3,072\n",
            "|    └─Tanh: 2-14                        [-1, 3, 64, 64]           --\n",
            "==========================================================================================\n",
            "Total params: 3,838,912\n",
            "Trainable params: 3,838,912\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 436.38\n",
            "==========================================================================================\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.97\n",
            "Params size (MB): 14.64\n",
            "Estimated Total Size (MB): 16.63\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "generator = Generator().to(device)\n",
        "summary(generator,torch.zeros(32,100,1,1).to(device), torch.zeros(32).long().to(device));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbCRkWizujcJ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99KnWQr9umUL"
      },
      "source": [
        "## Training Step and Objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ry14Ju7LpjCL"
      },
      "outputs": [],
      "source": [
        "def noise(size):\n",
        "    n = torch.randn(size, 100, 1, 1, device=device)\n",
        "    return n.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQLdvHvQpkyV"
      },
      "outputs": [],
      "source": [
        "def discriminator_train_step(real_data, real_labels, fake_data, fake_labels):\n",
        "    d_optimizer.zero_grad()\n",
        "    prediction_real = discriminator(real_data, real_labels)\n",
        "    error_real = loss(prediction_real, torch.ones(len(real_data), 1).to(device))\n",
        "    error_real.backward()\n",
        "    prediction_fake = discriminator(fake_data, fake_labels)\n",
        "    error_fake = loss(prediction_fake, torch.zeros(len(fake_data), 1).to(device))\n",
        "    error_fake.backward()\n",
        "    d_optimizer.step()\n",
        "    return error_real + error_fake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOO2Qr_KprEE"
      },
      "outputs": [],
      "source": [
        "def generator_train_step(fake_data, fake_labels):\n",
        "    g_optimizer.zero_grad()\n",
        "    prediction = discriminator(fake_data, fake_labels)\n",
        "    error = loss(prediction, torch.ones(len(fake_data), 1).to(device))\n",
        "    error.backward()\n",
        "    g_optimizer.step()\n",
        "    return error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBkGVuKKptJQ"
      },
      "outputs": [],
      "source": [
        "discriminator = Discriminator().to(device)\n",
        "generator = Generator().to(device)\n",
        "\n",
        "loss = nn.BCELoss()\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "fixed_noise = torch.randn(64, 100, 1, 1, device=device)\n",
        "fixed_fake_labels = torch.LongTensor([0]*(len(fixed_noise)//2) + [1]*(len(fixed_noise)//2)).to(device)\n",
        "\n",
        "n_epochs = 25\n",
        "img_list = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lruwgJDupDk"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mD0T21Our8y"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DszoT7j5pvzS",
        "outputId": "d08bc7e4-2ccc-403b-ffc0-fca1497bf728"
      },
      "outputs": [],
      "source": [
        "# Initialize W&B\n",
        "run = wandb.init(project='W&B_Generate_Faces_using_ConditionalGAN',\n",
        "      config= WANDB_CONFIG)\n",
        "\n",
        "for epoch in tqdm(range(n_epochs), total = n_epochs):\n",
        "    N = len(dataloader)\n",
        "    for bx, (images, labels) in enumerate(dataloader):\n",
        "        real_data, real_labels = images.to(device), labels.to(device)\n",
        "        fake_labels = torch.LongTensor(np.random.randint(0, 2, len(real_data))).to(device)\n",
        "        fake_data = generator(noise(len(real_data)), fake_labels)\n",
        "        fake_data = fake_data.detach()\n",
        "        d_loss = discriminator_train_step(real_data, real_labels, fake_data, fake_labels)\n",
        "        fake_labels = torch.LongTensor(np.random.randint(0, 2, len(real_data))).to(device)\n",
        "        fake_data = generator(noise(len(real_data)), fake_labels).to(device)\n",
        "        g_loss = generator_train_step(fake_data, fake_labels)\n",
        "\n",
        "        wandb.log(\n",
        "            {\n",
        "                'd_loss':d_loss.detach(),\n",
        "                'g_loss':g_loss.detach()\n",
        "            }\n",
        "        )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        fake = generator(fixed_noise, fixed_fake_labels).detach().cpu()\n",
        "        imgs = vutils.make_grid(fake, padding=2, normalize=True).permute(1,2,0)\n",
        "        img_list.append(imgs)\n",
        "        show(imgs, sz=10)\n",
        "\n",
        "    if epoch == 24:\n",
        "      plt.figure(figsize=(10,10))\n",
        "      plt.axis(\"off\")\n",
        "      plt.imshow(imgs)\n",
        "      plt.savefig(\"conditional_gan_predictions.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22z8VhLhGsiw"
      },
      "outputs": [],
      "source": [
        "table_generated = wandb.Table(columns=['Image'], allow_mixed_types = True)\n",
        "\n",
        "table_generated.add_data(\n",
        "            wandb.Image(\"/content/conditional_gan_predictions.jpg\"),\n",
        "        )\n",
        "\n",
        "wandb.log({\"Generated Images by Conditional GAN\" : table_generated})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U68s5T_Jux0k"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmCcZRsBu8Z6"
      },
      "source": [
        "## Wandb Dashboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNfVRE0Nu954"
      },
      "source": [
        "The code metrics and logs can be found in [this](https://wandb.ai/ishandutta/W&B_Generate_Faces_using_ConditionalGAN?workspace=user-ishandutta) dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAYDy6UAvDu6"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
