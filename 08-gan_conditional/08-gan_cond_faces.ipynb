{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Computer Vision - Week_08 - Conditional GAN (dataset: faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daces_metadata = pd.read_csv(\"/home/mh731nk/_data/experiments_tmp/data/celeba_dataset/list_attr_celeba.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_id', '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive',\n",
       "       'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose',\n",
       "       'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows',\n",
       "       'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair',\n",
       "       'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open',\n",
       "       'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin',\n",
       "       'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns',\n",
       "       'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings',\n",
       "       'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace',\n",
       "       'Wearing_Necktie', 'Young'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_daces_metadata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>5_o_Clock_Shadow</th>\n",
       "      <th>Arched_Eyebrows</th>\n",
       "      <th>Attractive</th>\n",
       "      <th>Bags_Under_Eyes</th>\n",
       "      <th>Bald</th>\n",
       "      <th>Bangs</th>\n",
       "      <th>Big_Lips</th>\n",
       "      <th>Big_Nose</th>\n",
       "      <th>Black_Hair</th>\n",
       "      <th>...</th>\n",
       "      <th>Sideburns</th>\n",
       "      <th>Smiling</th>\n",
       "      <th>Straight_Hair</th>\n",
       "      <th>Wavy_Hair</th>\n",
       "      <th>Wearing_Earrings</th>\n",
       "      <th>Wearing_Hat</th>\n",
       "      <th>Wearing_Lipstick</th>\n",
       "      <th>Wearing_Necklace</th>\n",
       "      <th>Wearing_Necktie</th>\n",
       "      <th>Young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000003.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000004.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000005.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202594</th>\n",
       "      <td>202595.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202595</th>\n",
       "      <td>202596.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202596</th>\n",
       "      <td>202597.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202597</th>\n",
       "      <td>202598.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202598</th>\n",
       "      <td>202599.jpg</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202599 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          image_id  5_o_Clock_Shadow  Arched_Eyebrows  Attractive  \\\n",
       "0       000001.jpg                -1                1           1   \n",
       "1       000002.jpg                -1               -1          -1   \n",
       "2       000003.jpg                -1               -1          -1   \n",
       "3       000004.jpg                -1               -1           1   \n",
       "4       000005.jpg                -1                1           1   \n",
       "...            ...               ...              ...         ...   \n",
       "202594  202595.jpg                -1               -1           1   \n",
       "202595  202596.jpg                -1               -1          -1   \n",
       "202596  202597.jpg                -1               -1          -1   \n",
       "202597  202598.jpg                -1                1           1   \n",
       "202598  202599.jpg                -1                1           1   \n",
       "\n",
       "        Bags_Under_Eyes  Bald  Bangs  Big_Lips  Big_Nose  Black_Hair  ...  \\\n",
       "0                    -1    -1     -1        -1        -1          -1  ...   \n",
       "1                     1    -1     -1        -1         1          -1  ...   \n",
       "2                    -1    -1     -1         1        -1          -1  ...   \n",
       "3                    -1    -1     -1        -1        -1          -1  ...   \n",
       "4                    -1    -1     -1         1        -1          -1  ...   \n",
       "...                 ...   ...    ...       ...       ...         ...  ...   \n",
       "202594               -1    -1     -1         1        -1          -1  ...   \n",
       "202595               -1    -1      1         1        -1          -1  ...   \n",
       "202596               -1    -1     -1        -1        -1           1  ...   \n",
       "202597               -1    -1     -1         1        -1           1  ...   \n",
       "202598               -1    -1     -1        -1        -1          -1  ...   \n",
       "\n",
       "        Sideburns  Smiling  Straight_Hair  Wavy_Hair  Wearing_Earrings  \\\n",
       "0              -1        1              1         -1                 1   \n",
       "1              -1        1             -1         -1                -1   \n",
       "2              -1       -1             -1          1                -1   \n",
       "3              -1       -1              1         -1                 1   \n",
       "4              -1       -1             -1         -1                -1   \n",
       "...           ...      ...            ...        ...               ...   \n",
       "202594         -1       -1             -1         -1                -1   \n",
       "202595         -1        1              1         -1                -1   \n",
       "202596         -1        1             -1         -1                -1   \n",
       "202597         -1        1             -1          1                 1   \n",
       "202598         -1       -1             -1          1                -1   \n",
       "\n",
       "        Wearing_Hat  Wearing_Lipstick  Wearing_Necklace  Wearing_Necktie  \\\n",
       "0                -1                 1                -1               -1   \n",
       "1                -1                -1                -1               -1   \n",
       "2                -1                -1                -1               -1   \n",
       "3                -1                 1                 1               -1   \n",
       "4                -1                 1                -1               -1   \n",
       "...             ...               ...               ...              ...   \n",
       "202594           -1                 1                -1               -1   \n",
       "202595           -1                -1                -1               -1   \n",
       "202596           -1                -1                -1               -1   \n",
       "202597           -1                 1                -1               -1   \n",
       "202598           -1                 1                -1               -1   \n",
       "\n",
       "        Young  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  \n",
       "...       ...  \n",
       "202594      1  \n",
       "202595      1  \n",
       "202596      1  \n",
       "202597      1  \n",
       "202598      1  \n",
       "\n",
       "[202599 rows x 41 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_daces_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84434, 41)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_daces_metadata.loc[df_daces_metadata[\"Male\"]==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "batch_size = 265\n",
    "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5) # mean, std for normalize imagess\n",
    "latent_size = 128\n",
    "lr = 0.00025\n",
    "epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29983, 41)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_daces_metadata.loc[df_daces_metadata[\"Blond_Hair\"]==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41572, 41)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_daces_metadata.loc[df_daces_metadata[\"Brown_Hair\"]==1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = 'generated_lab_week08'\n",
    "os.makedirs(sample_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "  \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "  if isinstance(data, (list,tuple)):\n",
    "      return [to_device(x, device) for x in data]\n",
    "  return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_samples(index, latent_tensors, show=True):\n",
    "  fake_images = generator(latent_tensors)\n",
    "  fake_fname = 'generated=images-{0:0=4d}.png'.format(index)\n",
    "  save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)\n",
    "  print(\"Saving\", fake_fname)\n",
    "\n",
    "  if show:\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, nmax=64):\n",
    "  fig, ax = plt.subplots(figsize=(8,8))\n",
    "  ax.set_xticks([]); ax.set_yticks([])\n",
    "  ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
    "  \n",
    "def show_batch(dl, nmax=64):\n",
    "  for images, _ in dl:\n",
    "    show_images(images, nmax)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset prepair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/mh731nk/_data/experiments_tmp/data/celeba_dataset/img_align_celeba'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "# Load CSV file with labels\n",
    "csv_file = '/home/mh731nk/_data/experiments_tmp/data/celeba_dataset/list_attr_celeba.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, csv_file, transform=None):\n",
    "        self.image_folder = ImageFolder(root=root_dir, transform=None)  # Ensure ImageFolder returns PIL Images\n",
    "        self.labels_df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_folder)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, _ = self.image_folder.imgs[idx]\n",
    "        img = Image.open(img_path).convert('RGB')  # Open image as PIL Image\n",
    "        \n",
    "        image_id = img_path.split('/')[-1]  # Extract image_id from image path\n",
    "        labels = self.labels_df[self.labels_df['image_id'] == image_id].iloc[:, 1:].values.flatten()\n",
    "        labels = torch.IntTensor(labels)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, labels\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*stats)\n",
    "])\n",
    "\n",
    "# Create custom dataset and dataloader\n",
    "custom_dataset = CustomDataset(root_dir=DATA_DIR, csv_file=csv_file, transform=transform)\n",
    "custom_dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([265, 40])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images,labels \u001b[38;5;129;01min\u001b[39;00m custom_dataloader:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for images,labels in custom_dataloader:\n",
    "    print(labels.shape)\n",
    "    assert False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = ImageFolder(root=DATA_DIR, \n",
    "#             transform=T.Compose([T.Resize(image_size),\n",
    "#                                 T.CenterCrop(image_size), # pick central square crop of it\n",
    "#                                 T.ToTensor(),\n",
    "#                                 T.Normalize(*stats)        # normalize => -1 to 1                               \n",
    "#                             ]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True) # use multiple cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to denormalize the image tensors and display some sample images from a training batch. \n",
    "# In future for helping train could be calculated zero centered nomralization per channel :)\n",
    "def denorm(img_tensors):\n",
    "    \"Denormalize image tensor with specified mean and std\"\n",
    "    return img_tensors * stats[1][0] + stats[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, nmax=64):\n",
    "  fig, ax = plt.subplots(figsize=(8,8))\n",
    "  ax.set_xticks([]); ax.set_yticks([])\n",
    "  ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
    "  \n",
    "def show_batch(dl, nmax=64):\n",
    "  for images, _ in dl:\n",
    "    show_images(images, nmax)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_batch(custom_dataloader) # original data looks \"cudne pokrucene\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to GPU\n",
    "train_dl = DeviceDataLoader(custom_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_size, number_of_class):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.number_of_class = number_of_class\n",
    "        self.embeding_size = self.latent_size\n",
    "\n",
    "        self.label_embeddings = nn.Embedding(self.number_of_class,self.embeding_size)\n",
    "\n",
    "        self.conv1 = nn.ConvTranspose2d(self.latent_size + self.embeding_size, 512, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.conv2 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.conv3 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.conv4 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.conv5 = nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_latent, input_label):\n",
    "        print(input_latent.shape)\n",
    "        print(input_label.shape)\n",
    "        label_embeddings = self.label_embeddings(input_label).view(len(input_label), self.embeding_size, 1, 1)\n",
    "        print(input_label.shape)\n",
    "        input_cat = torch.cat([input_latent, label_embeddings], 1)\n",
    "        x = self.relu(self.bn1(self.conv1(input_cat)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.tanh(self.conv5(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, emb_size=128):\n",
    "        super(Generator,self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.label_embeddings = nn.Embedding(40, self.emb_size)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100+self.emb_size,64*8,4,1,0,bias=False),\n",
    "            nn.BatchNorm2d(64*8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64*8,64*4,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64*4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64*4,64*2,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64*2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64*2,64,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64,3,4,2,1,bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.apply(weights_init)\n",
    "\n",
    "\n",
    "    def forward(self,input_noise,labels):\n",
    "        label_embeddings = self.label_embeddings(labels).view(len(labels), self.emb_size, 1)\n",
    "        \n",
    "        input_x = torch.cat([input_noise, label_embeddings], 1)\n",
    "        return self.model(input_x)\n",
    "\n",
    "\n",
    "generator = Generator().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Discriminator, self).__init__()\n",
    "\n",
    "#         self.label_embeddings = nn.Embedding(2, self.emb_size)\n",
    "#         # Define the leaky ReLU activation\n",
    "#         self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "#         # Define the convolutional layers\n",
    "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "#         self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(128)\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "#         self.bn3 = nn.BatchNorm2d(256)\n",
    "\n",
    "#         self.conv4 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "#         self.bn4 = nn.BatchNorm2d(512)\n",
    "\n",
    "#         self.conv5 = nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         # Define the forward pass\n",
    "#         x = self.leaky_relu(self.bn1(self.conv1(input)))\n",
    "#         x = self.leaky_relu(self.bn2(self.conv2(x)))\n",
    "#         x = self.leaky_relu(self.bn3(self.conv3(x)))\n",
    "#         x = self.leaky_relu(self.bn4(self.conv4(x)))\n",
    "#         x = self.conv5(x)\n",
    "#         x = self.flatten(x)\n",
    "#         x = self.sigmoid(x)\n",
    "#         return x\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, number_of_class,embeding_size=128):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.number_of_class = number_of_class\n",
    "        self.embeding_size = embeding_size\n",
    "\n",
    "        self.label_embeddings = nn.Embedding(40, self.embeding_size )\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3,64,4,2,1,bias=False),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(64,64*2,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64*2),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(64*2,64*4,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64*4),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(64*4,64*8,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64*8),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(64*8,64,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.model2 = nn.Sequential(\n",
    "            nn.Linear(288,100),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Linear(100,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.apply(weights_init)\n",
    "\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        x = self.model(img)\n",
    "        y = self.label_embeddings(labels)\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "        z = torch.cat([x, y], 1)\n",
    "        final_output = self.model2(z)\n",
    "        return final_output\n",
    "\n",
    "\n",
    "discriminator = Discriminator(40,128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generator = Generator(latent_size, 41)\n",
    "# generator = Generator()\n",
    "# # self.apply(weights_init)\n",
    "# # generator = weights_init(generator)\n",
    "# discriminator = Discriminator(41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight Initialization\n",
    "We define the weight initialization so that we do not have a widespread variation across randomly initialized weight values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# summary(generator, (128,1,1))\n",
    "# print(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((1,41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(size):\n",
    "    n = torch.randn(size, 100, 1, 1, device=device)\n",
    "    return n.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_train_step(real_data, real_labels, fake_data, fake_labels):\n",
    "    d_optimizer.zero_grad()\n",
    "\n",
    "    prediction_real = discriminator(real_data, real_labels)\n",
    "    error_real = loss(prediction_real, torch.ones(len(real_data), 1).to(device))\n",
    "    error_real.backward()\n",
    "\n",
    "    prediction_fake = discriminator(fake_data, fake_labels)\n",
    "    error_fake = loss(prediction_fake, torch.zeros(len(fake_data), 1).to(device))\n",
    "    error_fake.backward()\n",
    "    \n",
    "    d_optimizer.step()    \n",
    "    return error_real + error_fake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_train_step(fake_data, fake_labels):\n",
    "    g_optimizer.zero_grad()\n",
    "\n",
    "    prediction = discriminator(fake_data, fake_labels)\n",
    "\n",
    "    error = loss(prediction, torch.ones(len(fake_data), 1).to(device))\n",
    "    \n",
    "    error.backward()\n",
    "    g_optimizer.step()\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install  torch_snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_snippets import *\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "from torch_snippets import *\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">cuda\n",
       "</pre>\n"
      ],
      "text/plain": [
       "cuda\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(device)\n",
    "\n",
    "# generator = Generator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()\n",
    "\n",
    "\n",
    "\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "# fixed_noise will be used to generate images from random noise\n",
    "fixed_noise = torch.randn(64, 100, 1, 1, device=device)\n",
    "\n",
    "\n",
    "# Half labels correspond to class 0 and remaining to class 1\n",
    "fixed_fake_labels = torch.LongTensor([0]*(len(fixed_noise)//2) + [1]*(len(fixed_noise)//2)).to(device)\n",
    "\n",
    "\n",
    "n_epochs = 25\n",
    "img_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fixed_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 41, 1, 1])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor(np.random.randint(0, 2, (256,41,1,1))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">cuda\n",
       "</pre>\n"
      ],
      "text/plain": [
       "cuda\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">real_data  <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.Tensor'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "real_data  \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.Tensor'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">265</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m265\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m64\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">-----\n",
       "</pre>\n"
      ],
      "text/plain": [
       "-----\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">real_labels  <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.Tensor'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "real_labels  \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.Tensor'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">265</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m265\u001b[0m, \u001b[1;36m40\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">fake_labels  <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.Tensor'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "fake_labels  \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.Tensor'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">265</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m265\u001b[0m, \u001b[1;36m40\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[265, 128, 1]' is invalid for input of size 1356800",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake_labels \u001b[39m\u001b[38;5;124m\"\u001b[39m ,\u001b[38;5;28mtype\u001b[39m(fake_labels))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(fake_labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 21\u001b[0m fake_data \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreal_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# fake_data = fake_data.detach()\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Train discriminator\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake_data \u001b[39m\u001b[38;5;124m\"\u001b[39m ,\u001b[38;5;28mtype\u001b[39m(fake_data))\n",
      "File \u001b[0;32m~/anaconda3/envs/mh731nk_apvv-lung-eval/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[61], line 66\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, input_noise, labels)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,input_noise,labels):\n\u001b[0;32m---> 66\u001b[0m     label_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memb_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     input_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([input_noise, label_embeddings], \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(input_x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[265, 128, 1]' is invalid for input of size 1356800"
     ]
    }
   ],
   "source": [
    "# Train the model for 25 epochs\n",
    "for epoch in tqdm(range(n_epochs), total = n_epochs):\n",
    "    N = len(custom_dataloader)\n",
    "\n",
    "\n",
    "    for bx, (images, labels) in enumerate(custom_dataloader):\n",
    "\t    # Obtain the data\n",
    "        real_data, real_labels = images.to(device), labels.to(device)\n",
    "\n",
    "        fake_labels = torch.LongTensor(np.random.randint(0, 2, (len(real_data), 40))).to(device)\n",
    "\n",
    "        print(device)\n",
    "        print(\"real_data \" , type(real_data))\n",
    "        print(real_data.shape)\n",
    "\n",
    "        print(\"-----\")\n",
    "        print(\"real_labels \" ,type(real_labels))\n",
    "        print(real_labels.shape)\n",
    "\n",
    "        print(\"fake_labels \" ,type(fake_labels))\n",
    "        print(fake_labels.shape)\n",
    "\n",
    "\n",
    "        fake_data = generator(noise(len(real_data)), fake_labels)\n",
    "        # fake_data = fake_data.detach()\n",
    "\n",
    "\n",
    "        # Train discriminator\n",
    "       \n",
    "        print(\"fake_data \" ,type(fake_data))\n",
    "        print(fake_data.shape)\n",
    "\n",
    "\n",
    "        # d_loss = discriminator_train_step(real_data, real_labels.to(torch.int64), fake_data, fake_labels)\n",
    "\n",
    "        d_loss = discriminator_train_step(real_data, real_labels, fake_data, fake_labels)\n",
    "        \n",
    "        \n",
    "        fake_labels = torch.LongTensor(np.random.randint(0, 2, len(real_data))).to(device)\n",
    "\n",
    "        # Train generator\n",
    "        fake_data = generator(noise(len(real_data)), fake_labels).to(device)\n",
    "        \n",
    "        g_loss = generator_train_step(fake_data, fake_labels)\n",
    "\n",
    "\t# Log to wandb\n",
    "        # wandb.log(\n",
    "        #     {\n",
    "        #         'd_loss':d_loss.detach(),\n",
    "        #         'g_loss':g_loss.detach()\n",
    "        #     }\n",
    "        # )\n",
    "\t\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        fake = generator(fixed_noise, fixed_fake_labels).detach().cpu()\n",
    "        imgs = vutils.make_grid(fake, padding=2, normalize=True).permute(1,2,0)\n",
    "        img_list.append(imgs)\n",
    "        \n",
    "        show(imgs, sz=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Discriminator, self).__init__()\n",
    "\n",
    "#         self.label_embeddings = nn.Embedding(2, self.emb_size)\n",
    "#         # Define the leaky ReLU activation\n",
    "#         self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "#         # Define the convolutional layers\n",
    "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "#         self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(128)\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "#         self.bn3 = nn.BatchNorm2d(256)\n",
    "\n",
    "#         self.conv4 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "#         self.bn4 = nn.BatchNorm2d(512)\n",
    "\n",
    "#         self.conv5 = nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         # Define the forward pass\n",
    "#         x = self.leaky_relu(self.bn1(self.conv1(input)))\n",
    "#         x = self.leaky_relu(self.bn2(self.conv2(x)))\n",
    "#         x = self.leaky_relu(self.bn3(self.conv3(x)))\n",
    "#         x = self.leaky_relu(self.bn4(self.conv4(x)))\n",
    "#         x = self.conv5(x)\n",
    "#         x = self.flatten(x)\n",
    "#         x = self.sigmoid(x)\n",
    "#         return x\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, number_of_class,embeding_size=128,):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.number_of_class = number_of_class\n",
    "        self.embeding_size = embeding_size\n",
    "\n",
    "        self.label_embeddings = nn.Embedding(41, self.embeding_size )\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3,64,4,2,1,bias=False),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(64,64*2,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64*2),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(64*2,64*4,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64*4),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(64*4,64*8,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64*8),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(64*8,64,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.model2 = nn.Sequential(\n",
    "            nn.Linear(288,100),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Linear(100,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.apply(weights_init)\n",
    "\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        x = self.model(input)\n",
    "        y = self.label_embeddings(labels)\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "        input = torch.cat([x, y], 1)\n",
    "        final_output = self.model2(input)\n",
    "        return final_output\n",
    "\n",
    "\n",
    "discriminator = Discriminator(41).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 265 but got size 41 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m xb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# random latent tensors\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m fake_images \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m41\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# print(fake_images.shape)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# show_images(fake_images)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mh731nk_apvv-lung-eval/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[28], line 67\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, input_noise, labels)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,input_noise,labels):\n\u001b[1;32m     66\u001b[0m     label_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_embeddings(labels)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_embeddings\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 265 but got size 41 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "xb = torch.randn(batch_size, 100, 1, 1) # random latent tensors\n",
    "fake_images = generator(xb, torch.ones((41,1,1), dtype=torch.int))\n",
    "# print(fake_images.shape)\n",
    "# show_images(fake_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00025"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = to_device(generator, device) \n",
    "discriminator = to_device(discriminator, device)\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epochs, lr, start_idx = 1):\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "  # Losses & scores\n",
    "  losses_g = []\n",
    "  losses_d = []\n",
    "  real_scores = []\n",
    "  fake_scores = []\n",
    "\n",
    "  # Create optimizers\n",
    "  opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "  opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "  for idx, epoch in enumerate(range(0,epochs)):\n",
    "    print(f\"epoch {idx} start\")\n",
    "    for real_images, _ in tqdm(train_dl):\n",
    "\n",
    "\n",
    "        ### Train discriminator\n",
    "        # Clear discriminator gradients\n",
    "        opt_d.zero_grad()\n",
    "\n",
    "        # Pass real images through  discriminator\n",
    "        real_preds = discriminator(real_images)\n",
    "        real_targets = torch.ones(real_images.size(0), 1, device=device)\n",
    "        real_loss = F.binary_cross_entropy(real_preds, real_targets)\n",
    "        real_score = torch.mean(real_preds).item()\n",
    "\n",
    "        # Generate fake images\n",
    "        latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
    "        fake_images = generator(latent)\n",
    "\n",
    "        # Pass Fake images through discriminator\n",
    "        fake_targets = torch.zeros(fake_images.size(0), 1, device=device)\n",
    "        fake_preds = discriminator(fake_images)\n",
    "        fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n",
    "        fake_score = torch.mean(fake_preds).item()\n",
    "\n",
    "        # Update discriminator weights\n",
    "        loss_d = real_loss + fake_loss\n",
    "        loss_d.backward()\n",
    "        opt_d.step()\n",
    "\n",
    "        # loss_d, real_score, fake_score = train_discriminator(real_images, opt_d)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### Train generator\n",
    "        # Clear generator gradients\n",
    "        opt_g.zero_grad()\n",
    "\n",
    "        # Generate fake images\n",
    "        latent = torch.randn(batch_size, latent_size, 1,1, device=device)\n",
    "        fake_images = generator(latent)\n",
    "\n",
    "        # Try to fool the discriminator\n",
    "        preds = discriminator(fake_images)\n",
    "        targets = torch.ones(batch_size, 1, device=device)\n",
    "        loss_g = F.binary_cross_entropy(preds, targets)\n",
    "\n",
    "        # Update generator \n",
    "        loss_g.backward()\n",
    "        opt_g.step()\n",
    "\n",
    "\n",
    "\n",
    "    # Record losses & scores\n",
    "    losses_g.append(loss_g)\n",
    "    losses_d.append(loss_d.item())\n",
    "    real_scores.append(real_score)\n",
    "    fake_scores.append(fake_score)\n",
    "\n",
    "    # Log losses & scores (last batch)\n",
    "    print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n",
    "    # Save generated images\n",
    "    save_samples(epoch+start_idx, fixed_latent, show=False)\n",
    "\n",
    "  return losses_g, losses_d, real_scores, fake_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test latent space\n",
    "We'll use a fixed set of input vectors to the generator to see how the individual generated images evolve over time as we train the model. Let's save one set of images before we start training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_latent = torch.randn(64, latent_size, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 532/765 [00:29<00:12, 18.72it/s]"
     ]
    }
   ],
   "source": [
    "history = train(epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
